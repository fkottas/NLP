{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wrPgbfNY8cu"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pn5k8tvLY8cx"
      },
      "outputs": [],
      "source": [
        "STUDENT_ID = \"128\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxm6KqPmY8cz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7b71d2b1e1345af2935bd2b0ad589ba4",
          "grade": false,
          "grade_id": "cell-a08dd0da19414a7f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Jl0jfCLyY8c0"
      },
      "source": [
        "Your task is to create an RNN using TensorFlow to classify news headlines into one of four categories: World News, Sports, Business, or Sci/Tech. Study first the example given in the Recurrent Neural Networks and the Encoder-Decoder Architecture section in the elearning platform: https://colab.research.google.com/drive/1dA0mXy8nwvnxPB-51LIKV7LBjIkOlMkF?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b55ecde6835f6326194242b73e2802c0",
          "grade": false,
          "grade_id": "cell-e65efc47faca357c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIkiB9PmY8c1",
        "outputId": "2072c0da-73e0-40c1-f474-73d40b09f720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.17.1\n",
            "Eager mode:  True\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3578789d18e9202955b64625891b98e4",
          "grade": false,
          "grade_id": "cell-25e45b86e176af33",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "fyCH82ZLY8c2"
      },
      "source": [
        "Load the AG News Subset dataset using tfds. Split the existing train split into half, in order to use the first half for training and the second half for validation. Store the training, validation and test splits into variables train_dataset, validation_dataset and test_dataset respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0ed345d3dd05ccb98bebdc4557e8fd36",
          "grade": false,
          "grade_id": "cell-1f6327f4e44d0c2b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfjNRaHmY8c3",
        "outputId": "582c6184-341e-468c-cb25-0e4997f8eb6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 60000\n",
            "Number of validation examples: 60000\n",
            "Number of test examples: 7600\n"
          ]
        }
      ],
      "source": [
        "# Load the AG News Subset dataset\n",
        "dataset, info = tfds.load(\n",
        "    'ag_news_subset',\n",
        "    split=['train', 'test'],\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "train_val_dataset, test_dataset = dataset\n",
        "\n",
        "# Split the train dataset into training and validation subsets\n",
        "train_dataset = train_val_dataset.take(len(train_val_dataset) // 2)\n",
        "validation_dataset = train_val_dataset.skip(len(train_val_dataset) // 2)\n",
        "\n",
        "# Display dataset sizes\n",
        "train_size = len(list(train_dataset))\n",
        "val_size = len(list(validation_dataset))\n",
        "test_size = len(list(test_dataset))\n",
        "\n",
        "print(f\"Number of training examples: {train_size}\")\n",
        "print(f\"Number of validation examples: {val_size}\")\n",
        "print(f\"Number of test examples: {test_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f8e7000a315115f8b465481462deee7f",
          "grade": true,
          "grade_id": "cell-4cdaf5a0e50e9e29",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1GDbFFcYY8c4"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert len(train_dataset) == 60000\n",
        "assert len(validation_dataset) == 60000\n",
        "assert len(test_dataset) == 7600"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH_aVD0mY8c5"
      },
      "source": [
        "Batch the datasets to be ready for training and inference. Use a batch size of 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7d0ba4a5b5d99044819731bdebeb64c8",
          "grade": false,
          "grade_id": "cell-7011f80019607313",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "e9K8boC0Y8c6"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 5000\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7900dd46c819288d130fd10e3b77a71c",
          "grade": true,
          "grade_id": "cell-2c7323704caceecc",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vkkabYXkY8c6"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "for batch in train_dataset.take(1):\n",
        "  inputs, labels = batch\n",
        "  assert inputs.shape[0] == 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyK3oW59Y8c7"
      },
      "source": [
        "Define a TextVectorization layer with an appropriate output mode for use with RNNs and learn the vocabulary based on the training set using the adapt function. Use a 9k vocabulary size. Store the layer into a variable called encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9daf594e4e905c0c73f2568df897764f",
          "grade": false,
          "grade_id": "cell-ffb48d0dd0ef9ddb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt9VgGFIY8c7",
        "outputId": "04e73190-239f-480e-902c-55a133366133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextVectorization layer created and adapted to the training set.\n"
          ]
        }
      ],
      "source": [
        "# Define the TextVectorization layer\n",
        "VOCAB_SIZE = 9000\n",
        "\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,      # Maximum vocabulary size\n",
        "    output_mode='int',\n",
        "    output_sequence_length=100   #for faster training\n",
        ")\n",
        "\n",
        "# Adapt the encoder to the training set\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "print(\"TextVectorization layer created and adapted to the training set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a2cebeabff56e5f5c19875f9c77b10c8",
          "grade": true,
          "grade_id": "cell-26f6934d5ee0b2c6",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SQZWsFxUY8c7"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert len(encoder.get_vocabulary()) == 9000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hJ5vRrIY8c8"
      },
      "source": [
        "Construct the model architecture. Use a trainable embedding layer with 300 dimensions. Then use an LSTM layer with 128 cells. Finally use an appropriate output layer for the classification task at hand. Store the model in a variable called model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5a406d12ecbe61b908392ec879eedfbb",
          "grade": false,
          "grade_id": "cell-e38d673272fd7828",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "8B0bncHbY8c8",
        "outputId": "0c1d68ed-c6da-411e-b3c8-86176f0561d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m2,700,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m219,648\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m516\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,700,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">219,648</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,920,164\u001b[0m (11.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,920,164</span> (11.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,920,164\u001b[0m (11.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,920,164</span> (11.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Construct the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    # Input layer to define the shape explicitly\n",
        "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "    encoder,  # Text Vectorization layer\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),  # Vocabulary size\n",
        "        output_dim=300,                          # Embedding dimensions\n",
        "        mask_zero=True                           # Mask padding tokens\n",
        "    ),\n",
        "    # LSTM layer\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    # Dense output layer with softmax activation for multi-class classification\n",
        "    tf.keras.layers.Dense(4, activation='softmax')  # 4 categories\n",
        "])\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c28bc3ce580b17f478ee481ade682ae0",
          "grade": true,
          "grade_id": "cell-88464e33dd594690",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IRRlGYtRY8c8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert len(model.layers) == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Fv1PNdY8c9"
      },
      "source": [
        "Compile the model using the appropriate loss function. Study the available loss functions here: https://www.tensorflow.org/api_docs/python/tf/keras/losses . Look particularly at the CategoricalCrossEntropy and SparseCategoricalCrossEntropy losses. Which one should you use given the representation of the target values in our dataset? The following code prints the target values for the first batch of training examples. Use the Adam optimizer with a learning rate of 0.00001. Use accuracy as metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwOVuWDFY8c9",
        "outputId": "8aeb4120-c5b9-409a-fb4a-6038cca9cac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[2 1 2 1 2 0 2 1 2 1 1 3 3 3 3 3 0 3 3 2 0 1 2 1 3 0 1 3 1 0 3 1 1 2 2 2 2\n",
            " 1 2 3 3 3 0 2 2 2 2 3 3 3 1 0 3 1 2 3 2 1 1 2 1 3 2 2], shape=(64,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for text, label in train_dataset.take(1):\n",
        "  print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "78e134e3e6b8de1f26eececb7b2f6a45",
          "grade": false,
          "grade_id": "cell-18fec871490b630d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Z5XGIR8NY8c9"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "888bc6226ae7cb015b0340658d27f5f4",
          "grade": true,
          "grade_id": "cell-1de891abdee9a39d",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "B0eNEe_JY8c-"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert model.optimizer.learning_rate == 0.00001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai9oPotwY8c-"
      },
      "source": [
        "Fit the model for 5 epochs, presenting the metrics also for the validation set at the end of each epoch. Save the fit resutls to a history variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "222102c657719cca5c149f3917f8bd6e",
          "grade": false,
          "grade_id": "cell-7658728d0a580178",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Q7H7PKY8c-",
        "outputId": "4c0b7186-d34b-4f13-a57a-fb44698ab8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 17ms/step - accuracy: 0.3465 - loss: 1.3811 - val_accuracy: 0.5072 - val_loss: 1.2238\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.5048 - loss: 1.0976 - val_accuracy: 0.5564 - val_loss: 0.9757\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.5971 - loss: 0.9307 - val_accuracy: 0.7189 - val_loss: 0.7972\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.7620 - loss: 0.7317 - val_accuracy: 0.8058 - val_loss: 0.6360\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8218 - loss: 0.5845 - val_accuracy: 0.8243 - val_loss: 0.5707\n",
            "Model compiled successfully.\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "print(\"Model compiled successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2932426b56242f7865c12cbd60308799",
          "grade": true,
          "grade_id": "cell-fb4fa68f582f5184",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "u7L3YLz6Y8c_"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "final_accuracy = history.history['accuracy'][-1]\n",
        "assert final_accuracy > 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvZ_parDY8c_"
      },
      "source": [
        "Develop a function to get the embeddings that the network learned and compute the similarity with a given word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a7922313f55cbc81b1acd3af318bc7a7",
          "grade": false,
          "grade_id": "cell-3a30f8f0e8802544",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhi1KTmQY8dA",
        "outputId": "1f126e3d-867b-42bd-efa1-962275df6023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'computer': ['software', 'technology', 'space', 'internet', 'web']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_most_similar(target_word, top_n):\n",
        "    \"\"\"\n",
        "    Finds the most similar words to a given target word based on learned embeddings.\n",
        "\n",
        "    Args:\n",
        "        target_word (str): The word to find similar words for.\n",
        "        top_n (int): The number of most similar words to return.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: The list of most similar words.\n",
        "    \"\"\"\n",
        "    # Retrieve the embedding layer from the model\n",
        "    embedding_layer = model.get_layer('embedding')\n",
        "    embedding_weights = embedding_layer.get_weights()[0]  # Shape: (vocab_size, embedding_dim)\n",
        "\n",
        "    # Get the index of the target word using the encoder\n",
        "    target_word_idx = encoder([target_word]).numpy()[0][0]  # Get the token index for the target word\n",
        "\n",
        "    # Check if target_word_idx is valid (target word must be in the vocabulary)\n",
        "    if target_word_idx >= len(embedding_weights) or target_word_idx == 0:\n",
        "        # Index 0 is typically reserved for padding token, which should not be used\n",
        "        raise ValueError(f\"Word '{target_word}' not in vocabulary or it's padding/unknown.\")\n",
        "\n",
        "    # Get the embedding of the target word\n",
        "    target_embedding = embedding_weights[target_word_idx]\n",
        "\n",
        "    # Compute similarities with all other embeddings\n",
        "    similarities = cosine_similarity(\n",
        "        target_embedding.reshape(1, -1),  # Target word embedding\n",
        "        embedding_weights                # All embeddings in the vocabulary\n",
        "    )[0]  # Flatten the result to a 1D array\n",
        "\n",
        "    # Get the top N most similar word indices (excluding the target word itself and padding)\n",
        "    similar_indices = np.argsort(similarities)[-top_n - 1:][::-1]  # Sort in descending order\n",
        "    similar_indices = [i for i in similar_indices if i != target_word_idx and i != 0]  # Exclude target word and padding\n",
        "\n",
        "    # Map indices to words using the vocabulary\n",
        "    vocabulary = encoder.get_vocabulary()\n",
        "    similar_words = [vocabulary[i] for i in similar_indices][:top_n]  # Map indices to words\n",
        "\n",
        "    return similar_words\n",
        "\n",
        "# Example usage:\n",
        "most_similar_words = find_most_similar(\"computer\", 5)\n",
        "print(f\"Words most similar to 'computer': {most_similar_words}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "632bbdafa9a94a172c475b774eab1f63",
          "grade": true,
          "grade_id": "cell-8bb199def3e014cb",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "amlpWpDZY8dA"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing\"\"\"\n",
        "assert \"software\" in find_most_similar(\"computer\", 5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}