{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwuhMJ_Jf7Jy"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` removing 'raise NotImplementedError()'. Make usre you also fill your student id below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dy-Li4qOf7Jz"
      },
      "outputs": [],
      "source": [
       
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjb3NSt-f7J0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6piul4Pf7J1"
      },
      "source": [
        "## Part A: Regular expressions (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r-RURwSkf7J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ab6eda-24ad-4d00-e7c0-254bf2771a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import gutenberg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ5dGwbgf7J1"
      },
      "source": [
        "Complete the following function using the search function of the re library to find and return the year that a file in the gutenberg collection was written. Return the year as string or the empty string if not found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0101c0e26d604094cef8309d1990f3a3",
          "grade": false,
          "grade_id": "cell-6a452e90c99b682e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7QV5JsLLf7J1"
      },
      "outputs": [],
      "source": [
        "def get_year(file):\n",
        "    # raw text\n",
        "    text_raw = gutenberg.raw(file)\n",
        "\n",
        "    # The publication years are between 1000 and 2024\n",
        "    match = re.findall(r'\\b1\\d{3}|20[0-1]\\d{1}|202[0-4]\\b', text_raw)\n",
        "\n",
        "    # Return the first match found\n",
        "    if match:\n",
        "        return match[0]\n",
        "    # Return an empty string if no match is found\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8ded8656e634ec9676365a3d2425d81c",
          "grade": true,
          "grade_id": "cell-7011f80019607313",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "GkMuTwI6f7J1"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "assert get_year('carroll-alice.txt') == '1865'\n",
        "assert get_year('shakespeare-hamlet.txt') == '1599'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_year('carroll-alice.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D8DATNv0CcDh",
        "outputId": "5dfff0c3-9aca-45b4-f00b-c0b5a80eb59a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1865'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onW7VCovf7J1"
      },
      "source": [
        "Complete thw following function using the findall function of the re library to compute and report the number of times some text appears in a file of the gutenberg collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aa33bf40ffcefca144136b19837cd66f",
          "grade": false,
          "grade_id": "cell-fcd8fa5f85a0dbd7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "IogaxZwaf7J2"
      },
      "outputs": [],
      "source": [
        "def find_mentions(text, file):\n",
        "    # raw text\n",
        "    text_raw = gutenberg.raw(file)\n",
        "\n",
        "    # Count the text appearing in the file of the guternberg\n",
        "    match = re.findall(text, text_raw)\n",
        "\n",
        "    # Return the count of text\n",
        "    return len(match)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8e086cd31cac25809d39cb3749788e0f",
          "grade": true,
          "grade_id": "cell-b5c350fd93ad1d75",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7SeAsJwNf7J2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "assert find_mentions('CHAPTER ', 'carroll-alice.txt') == 12\n",
        "assert find_mentions('UNK', 'shakespeare-caesar.txt') == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu_-QMQzf7J2"
      },
      "source": [
        "Complete the following function to return the mean number of words in the sentences of a gutenberg file that is given as input. The number should be rounded to 2 decimals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "23ded46ba3aec1e38b55235406faed9a",
          "grade": false,
          "grade_id": "cell-4bc64ff6404ecf22",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "PcZa_GuFf7J2"
      },
      "outputs": [],
      "source": [
        "def average_sentence_length(file):\n",
        "    # have the sentences\n",
        "    sentences = gutenberg.sents(file)\n",
        "\n",
        "    # have the words\n",
        "    words = gutenberg.words(file)\n",
        "\n",
        "    # Calculate the mean number of words in the sentences of a gutenberg file\n",
        "    sent_length = round(len(words)/len(sentences),2)\n",
        "\n",
        "    # Return the result\n",
        "    return sent_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ba6ef24c27e66eda9f9707ec99b0a01e",
          "grade": true,
          "grade_id": "cell-27d42c9e86ec9169",
          "locked": true,
          "points": 0.5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "kmMakzPGf7J2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "assert average_sentence_length('shakespeare-caesar.txt') == 11.94\n",
        "assert average_sentence_length('austen-emma.txt') == 24.82"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWPyb1dof7J2"
      },
      "source": [
        "## Part B: Byte-Pair Encoding (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozMCNHj0f7J2"
      },
      "source": [
        "Implement a function that returns the vocabulary learned by the byte-pair encoding algorithm given a corpus, after a number of iterations. The characters in the corpus, sorted in alphabetical order, will be the first members of the vocabulary, to be extended with as many tokens as the number of iterations given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8e0afe3417c06a0a6113eef156f48e08",
          "grade": false,
          "grade_id": "cell-00f295323218bcab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7uDlMV5gf7J2"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def bpe_train(text, num_iters):\n",
        "# Tokenize the corpus, marking the end of each word with \"_\"\n",
        "    words = [word + \"_\" for word in corpus.split()]\n",
        "\n",
        "    # Initialize vocabulary with all unique characters in the corpus, sorted alphabetically\n",
        "    vocab = sorted(set(\"\".join(words)))\n",
        "\n",
        "    # Convert the words into lists of characters to facilitate merging\n",
        "    words = [list(word) for word in words]\n",
        "\n",
        "    # Run BPE for the specified number of iterations\n",
        "    for _ in range(num_iters):\n",
        "        # Count frequency of each adjacent pair in the corpus\n",
        "        pairs = Counter()\n",
        "        for word in words:\n",
        "            for i in range(len(word) - 1):\n",
        "                pair = (word[i], word[i + 1])\n",
        "                pairs[pair] += 1\n",
        "\n",
        "        # If no pairs are left to merge, break early\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Find the most frequent pair\n",
        "        most_frequent_pair = max(pairs, key=pairs.get)\n",
        "        new_token = \"\".join(most_frequent_pair)\n",
        "        vocab.append(new_token)\n",
        "\n",
        "        # Merge the most frequent pair in each word\n",
        "        new_words = []\n",
        "        for word in words:\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                # Check if we can merge the current pair\n",
        "                if i < len(word) - 1 and (word[i], word[i + 1]) == most_frequent_pair:\n",
        "                    new_word.append(new_token)\n",
        "                    i += 2  # Skip over the pair\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_words.append(new_word)\n",
        "        words = new_words\n",
        "\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "de84b3d1311f2c94545fc2d2d3dcab76",
          "grade": true,
          "grade_id": "cell-9be88b50d1385e55",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Gj63AyCUf7J3"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "corpus = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\"\n",
        "assert set(bpe_train(corpus, 8)) == set(['_', 'd', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'er', 'er_', 'ne', 'new', 'lo', 'low', 'newer_', 'low_']\n",
        ")\n",
        "corpus = \"hello world my large language model is coming after you beware\"\n",
        "assert set(bpe_train(corpus, 5)) == set(['_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'w', 'y', 'e_', 'el', 'la', 'ge_', 'ng']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbx9Ez6ff7J3"
      },
      "source": [
        "## Part C: Minimum Edit Distance (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-_cGcIFf7J3"
      },
      "source": [
        "Extend the code of the minimum edit distance algorithm to implement the following function that returns a list of at least one alignment. Each alignment should be a list containing the source string with stars for insertion, the target string with stars for deletion, and the path of operations consisting of characters s, i, d and - for substitution, insertion, deletion, and nothing respectively. See the testing cell for examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "625fda1b7332ee2c14e86ef72b8168ac",
          "grade": false,
          "grade_id": "cell-5ace2632c1f75716",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "osbNu2DZf7J3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_alignments(seq1, seq2, substitution_cost=1):\n",
        "    m, n = len(seq1), len(seq2)\n",
        "\n",
        "    # Initialize DP and backtrack tables using numpy\n",
        "    dp = np.zeros((m + 1, n + 1), dtype=int)\n",
        "    backtrack = np.full((m + 1, n + 1), None)\n",
        "\n",
        "    # Fill initial costs for converting prefixes\n",
        "    dp[:, 0] = np.arange(m + 1)    # Cost of deletions\n",
        "    dp[0, :] = np.arange(n + 1)    # Cost of insertions\n",
        "    backtrack[1:, 0] = 'd'         # Mark deletions in backtrack\n",
        "    backtrack[0, 1:] = 'i'         # Mark insertions in backtrack\n",
        "\n",
        "    # Fill DP and backtrack arrays\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost_substitute = dp[i - 1, j - 1] + (substitution_cost if seq1[i - 1] != seq2[j - 1] else 0)\n",
        "            cost_insert = dp[i, j - 1] + 1\n",
        "            cost_delete = dp[i - 1, j] + 1\n",
        "\n",
        "            # Select minimum cost and operation\n",
        "            min_cost = min(cost_substitute, cost_insert, cost_delete)\n",
        "            dp[i, j] = min_cost\n",
        "            if min_cost == cost_substitute:\n",
        "                backtrack[i, j] = 's' if seq1[i - 1] != seq2[j - 1] else '-'\n",
        "            elif min_cost == cost_insert:\n",
        "                backtrack[i, j] = 'i'\n",
        "            else:\n",
        "                backtrack[i, j] = 'd'\n",
        "\n",
        "    # Backtracking to get one or more optimal alignments\n",
        "    def backtrack_alignments(i, j):\n",
        "        if i == 0 and j == 0:\n",
        "            return [(\"\", \"\", \"\")]  # Base case: empty alignment\n",
        "\n",
        "        alignments = []\n",
        "        if i > 0 and backtrack[i, j] == 'd':\n",
        "            for prev_seq1, prev_seq2, prev_ops in backtrack_alignments(i - 1, j):\n",
        "                alignments.append((prev_seq1 + seq1[i - 1], prev_seq2 + '*', prev_ops + 'd'))\n",
        "\n",
        "        if j > 0 and backtrack[i, j] == 'i':\n",
        "            for prev_seq1, prev_seq2, prev_ops in backtrack_alignments(i, j - 1):\n",
        "                alignments.append((prev_seq1 + '*', prev_seq2 + seq2[j - 1], prev_ops + 'i'))\n",
        "\n",
        "        if i > 0 and j > 0:\n",
        "            if backtrack[i, j] == 's':\n",
        "                for prev_seq1, prev_seq2, prev_ops in backtrack_alignments(i - 1, j - 1):\n",
        "                    alignments.append((prev_seq1 + seq1[i - 1], prev_seq2 + seq2[j - 1], prev_ops + 's'))\n",
        "            elif backtrack[i, j] == '-':\n",
        "                for prev_seq1, prev_seq2, prev_ops in backtrack_alignments(i - 1, j - 1):\n",
        "                    alignments.append((prev_seq1 + seq1[i - 1], prev_seq2 + seq2[j - 1], prev_ops + '-'))\n",
        "\n",
        "        return alignments\n",
        "\n",
        "    # Get the alignments starting from (m, n)\n",
        "    all_alignments = backtrack_alignments(m, n)\n",
        "\n",
        "    # Format alignments for output\n",
        "    formatted_alignments = []\n",
        "    for seq1_aligned, seq2_aligned, ops in all_alignments:\n",
        "        formatted_alignments.append([seq1_aligned, seq2_aligned, ops])\n",
        "\n",
        "    return formatted_alignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d43dc3a6ff5924604a5a13e713dc86aa",
          "grade": true,
          "grade_id": "cell-a83f5648c3a7c7ec",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "38utBsAyf7J3"
      },
      "outputs": [],
      "source": [
        "\"\"\"Testing that the function returns the correct results\"\"\"\n",
        "for alignment in compute_alignments('nuts', 'bolts', 1):\n",
        "    assert alignment in [['*nuts', 'bolts', 'iss--'], ['n*uts', 'bolts', 'sis--'], ['nu*ts', 'bolts', 'ssi--']]\n",
        "for alignment in compute_alignments('ab', 'db', 2):\n",
        "    assert alignment in [['ab', 'db', 's-'], ['*ab', 'd*b', 'id-'], ['a*b', '*db', 'di-']]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
